{
	"next_topic_id": 1,
	"topic": [],
	"topic_vote": {},
	"next_comment_id": 38,
	"comment": {
		"37_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": [
			{
				"comment_id": 1,
				"body": "test, test",
				"added": 1468218949
			}
		],
		"7_144GQ5Uz9dYQJ1MwjSFWWboLgHwdGpv21x": [
			{
				"comment_id": 2,
				"body": "我其实对未来的互联网设想过，大概如此：基于远程无线电通信，每个网络设备本身即是terminal又是router, 网络层推倒重来，采用flat的mesh这种拓扑, 抛弃IP地址的概念，而采用某种session内有效的token来标识设备（从根本上实现匿名）。 想象一下，以后每个人手中的手机都可以与周围几公里甚至更远的人直接通信，如此连接而成的网络，应当很难封锁吧，那时信息的流动会更自由，高效。",
				"added": 1468238683
			}
		],
		"8_1DzpF7398yNGjFwHQqHAu9fRkbE3WZ6dZA": [
			{
				"comment_id": 3,
				"body": "we need to make it worse, so that the goverment has to shut it down, then BOOM, backfires. 置之死地而后生",
				"added": 1475254118
			}
		],
		"51_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": [
			{
				"comment_id": 4,
				"body": "从zerome贴过来的：\n现在的问题是数据冗余过大，按每个hub来seed粒度不够细，弊端其一是伸缩性差，其二就是hub之间是相对隔离的。我以为应当打破hub的隔离，以更细粒度来分发内容，开始的时候每个用户开始只需要强制seed部分（大多用户seed的内容不同，但整体保证足够冗余），和全局最近的posts，当用户浏览到未下载内容时再向peers请求。\n其实大多数数据是过往数据，并不需要每个用户都存储所有，所以把过往数据分发到所有zerome用户手中，每个用户只需要额外下载一定量的最近posts即可。",
				"added": 1481386911
			},
			{
				"comment_id": 5,
				"body": "> [p2p](#comment_994_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): hub是方便管理，以及去中心化。\n我认为，p2p理想状态是不需要管理的，因为有了足够好的伸缩性，可以应对数据过多之类的变化。我认为是否分hub与去中心化关系不是很大，因为每个人都只对自己的posts有绝对掌控，这就足够了，你可能考虑到如果不分hub，那么zerome是存在唯一管理者的，我认为只要他不乱来就不会有问题，比如交给nofish。",
				"added": 1481387420
			},
			{
				"comment_id": 6,
				"body": "实现这个设想，关键需要做到的是：\n1）控制冗余，保证所有数据的可访问性，即以一定冗余率平均地把所有zerome的数据分发到各个在线的节点上。\n2）最近posts的同步，可以考虑每个用户存储单独的文件为recentposts,每当一个节点发送新的消息时，向zerome网络中所有节点广播，各个节点更新各自的的recent posts文件，每个用户可以设置一个愿意接收的最近posts的量，在更新时砍掉旧的内容。\n3）用户自身与自身相关的posts全部都下载\n\n做到如上3点的话，应当在zerome主页用户不会看到太多变化，并且所有用户的信息是同步的，不存在没有加入hub而看不到信息的情况，也不存在多个hub多个身份的情况。",
				"added": 1481388179
			},
			{
				"comment_id": 7,
				"body": "> [lmath](#comment_288_1958F7oCppj78MP966AfojMQwHg2WUupzq): \n> 不同的人能只接收自己喜欢的内容\n可以通过follow功能来选择自己喜欢的内容。另外可以引入屏蔽用户的功能来辅助。",
				"added": 1481388298
			},
			{
				"comment_id": 8,
				"body": "> [p2p](#comment_995_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 就算nofish是个完美的管理员，但是他可能忙不过来啊，要搞开发要删spam，如果是他看不懂的语言，更晕\n\nspam不用删，让用户自己选择屏蔽，屏蔽用户的功能可以弥补这点。管理员其实不需要做太多维护的工作，只需要增加新的功能之类的，保证其继续运作，其实没有管理员，只要功能完善应该也是可以正常运作的。",
				"added": 1481388449
			},
			{
				"comment_id": 9,
				"body": "> [lmath](#comment_289_1958F7oCppj78MP966AfojMQwHg2WUupzq): 这种中心化控制还容易让 nofish 在一些情况下陷入危险境地，毕竟搞定了 nofish 就可以干掉 ZeroMe 上不想让人看见的所有内容了。\n然而分hub的机制并无法避免这一点，只不过是把目标增多了而已，而且不够多，设想一下，如果有足够多的hub，那么每个用户看到的内容都是很分散的，如果人不够多，则很难形成规模。",
				"added": 1481388652
			},
			{
				"comment_id": 10,
				"body": "> [p2p](#comment_996_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 大量spam仍然会占用网络和存储资源\n\n是的，但是所有流量会被摊分到所有节点上，所以不会有太大变化，并且recentposts只存在一定时间",
				"added": 1481388796
			},
			{
				"comment_id": 11,
				"body": "> [lmath](#comment_290_1958F7oCppj78MP966AfojMQwHg2WUupzq): 我也不认为分 Hub 很好，但是我反对的主要是一点，就是中心化管理，实际上 Hub 作为一个存储容器也没什么需要管的事（或许需要随着更新调整数据结构？）。\n\n> [p2p](#comment_997_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 不能极端化地设想。社区内自然会平衡的，要想建立hub，别人要愿意加入才行。事实上zerome运行了这么久，开放建hub，但有人用的也就是那几个hub\n\n不知是否可以剥夺管理者对用户数据修改的权利。即只能更新zite自身，但不能改变用户数据。\n这应该是普遍存在的一个问题，在zerotalk之类的论坛也类似。",
				"added": 1481389477
			},
			{
				"comment_id": 12,
				"body": "> [p2p](#comment_998_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 至少在现有架构上，需要管理员提升限额\n对了，有关限额，如果是所有用户分摊数据的话，每个用户的限额可以根据自身分摊的量来自动适配。其实如此一来，其实管理员需要的很多权限可以转移给每个用户。",
				"added": 1481390125
			},
			{
				"comment_id": 13,
				"body": "> [p2p](#comment_1000_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 提升限额、升级后端数据架构怎么办\n不要直接修改数据，先保证不修改原有数据，升级架构要兼容原有数据，只是增加新的数据，如此只需要更改代码，如加入增加新的数据项目的逻辑，管理员再push这个请求。\n\n> 用户对贡献量作弊怎么办？如果引入反作弊机制，越搞越复杂，值不值？\n现在用户的限额数据是在每个用户手里都有一份拷贝（hub-hash/content.json)，我想应该是在用户push时在peer处验证的。同理，用户的贡献量可以存放在其他peer里。初始状态可以是每个用户存储的数据是通过某种算法平均分摊的，在某范围内用户可以各自调整，调整时，就会更新所有人的hub-hash/content.json中该用户的限额。当用户数很多时，这个限额列表可能会很大（有很多人都更改了限额），到时可以再考虑把这个列表也分散到节点中去。",
				"added": 1481423812
			},
			{
				"comment_id": 14,
				"body": "> [p2p](#comment_1001_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 兼容的升级还是需要权限啊\n我的意思是，在代码上保证这个升级请求是只增不改，用户都更新代码后，就可以应对这个更新架构的请求。\n> 这样能反作弊？\n可能我有的地方说错了，我的意思是，把贡献量与限额关联，当用户提高贡献量（即限额）时，他会请求seed更多的数据，此时，在其他peer处就可以修改他的限额，并把这个限额数据同步到未波及的peers中。",
				"added": 1481424716
			},
			{
				"comment_id": 15,
				"body": "> [p2p](#comment_1002_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 打补丁式升级可能影响时间上、空间上的效率。比如说以后zeronet的数据要自带压缩了，不是一个个json了，怎么办？\n架构更新应该不是很频繁的，其实这与原先方式没有本质区别，只不过是在代码层面多了只加不改的限制，所以最初的数据结构需要设计的好些。\n> 问题是你怎么确定某节点贡献量大小，它是否作弊？\n在节点贡献时，传递给他数据的节点修改其限额，再push到所有节点。比如说节点A增加了自己的限额x，过一段时间后，A从B接收了数据y(y<=x)，那B那里对于A的限额就会改变为+y，此时B再向zerome上的其他节点更新这个变化。\n我又想了下，貌似最后更新这一步需要每个节点都有权限。",
				"added": 1481425339
			},
			{
				"comment_id": 16,
				"body": "> [lmath](#comment_293_1958F7oCppj78MP966AfojMQwHg2WUupzq): 不应该向其他节点更新这个节点限额变化，除非你能想到有效的办法限制 A 又搞了一个节点 B 来作弊（不贡献直接更新限额）。\n\n是的，可以把最后一步去掉，这样每个人在所有其他人那里就有不同的限额了。\n这样，假设整个zerome网络中数据的冗余率为x，那么每个节点向其他节点分发数据量y时，就修改其限额为+y/x。如此整个网络是平衡的。\n其实这样还是有很多问题，比如用户删除了所有数据，这样其他节点无法得知这个变化。限额数据无法同步地更新是个问题。",
				"added": 1481426266
			},
			{
				"comment_id": 17,
				"body": "> [p2p](#comment_993_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 我想到一个简单的方法：\n> \n> 所有用户文件夹内都加一个ping.json\n> 除了ping.json以外，其他全部是optional files\n> ping.json里存放该用户对哪些人评论过、@过（统称为ping过）\n> 当有人ping过你，你就下载Ta的data.json（当然，你fo的和你like的，自然也会下载其data.json）\n> 一个hub里，只有所有的ping.json是必须下载的，大大减少了所需空间和首次加载时间，并且不丢失社交网络的特性。\n> \n> EDIT：为了方便看你fo人的po下评论，还应该下载ping过Ta的人的data.json\n\n感觉有些像每个人一个hub，不过这样就看不到“Everyone\"的内容了，如果看不到everyone，那么就无法方便地认识新人，反过来能看到的内容就更少了。所以还是需要某种找到全局最近posts的机制。\n另外，说到去中心话，貌似每个人一个hub更好？这样每个人对自己的数据全权负责，他人无法插手，用户自由选择自己想seed的用户，没有限额限制。",
				"added": 1481545557
			},
			{
				"comment_id": 18,
				"body": "> [nekocross](#comment_118_1FHN4oukAG7zFUpiC9L2iz7QpQVTuT3xkz): 可以一个小团体一个hub，中文用户就可以分好几个，geek的，18x的，政治的\n\n所以就又回到如果hub变得过大怎么办的问题了。",
				"added": 1481550171
			},
			{
				"comment_id": 20,
				"body": "> [p2p](#comment_1014_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 每个人一个hub早就讨论过了，缺点是连接数太多，网络资源容易耗尽，而且在tor下连接数更有限。你的recent.json方案和我之前说的老贴放archive.json并无本质区别，只是文件名不同罢了。这种方案适合hub还不是太大时。我后来说的ping.json可以说是终极方案，hub人数10万+\n\n我说的recentposts只是方案的一部分，用来传达全局posts的，因为数据量是可控的（如最近的100条消息），所以与人数多少关系不大，而另一部分是把旧的posts平均分发到各节点，控制冗余率。",
				"added": 1481558090
			},
			{
				"comment_id": 21,
				"body": "> [p2p](#comment_1015_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 与人数多少关系不大？假设有10万人，你自己算\n人数多只是同步慢，不会对存储的数据量产生影响。\n我说的recentposts是指的所有人的（对于某个节点即他所了解的所有节点，直接地或间接地）。",
				"added": 1481560792
			},
			{
				"comment_id": 22,
				"body": "> [p2p](#comment_1016_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 为什么你就是不肯自己算呢？假设网站总大小限制在10MB，10万人，简单估算：每个人分得 10MB/100000=10000KB/100000=0.1KB=100Byte，请问100字节能装recentposts？\n你没理解我所说的recentposts的意思，这个recentposts是不分摊的，而是每个人共同的，以你的例子就是每个人都同步那10M（实际应该以消息条数作为标准），所以我说我的方案是两部分。\n这个的目的是为了展示那个\"Everyone\"的内容，现在的实现是seed hub里的所有数据，所以我们可以看到Everyone，而如果不seed所有人的话，要看到everyone，就需要改变机制，我提议的就是一种可能，以时间为标准同步最新的某x条posts。",
				"added": 1481600035
			},
			{
				"comment_id": 23,
				"body": "> [p2p](#comment_1017_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 所有人都可以往同一个recentposts.json里塞东西？你首先要理解zeronet的后端架构——每个人只对自己的子文件夹有写入权限，基于公私钥对的。\n\n这样是否可行，每条消息都由发送者自身sign，接收方看到消息中的发送人，用发送人的公钥验证后更新自己的recentposts，这样每个人存储所有人的公钥即可。\n即以消息为单位来验证，每个用户的recentposts文件是验证后消息的组合。",
				"added": 1481602116
			},
			{
				"comment_id": 24,
				"body": "> [p2p](#comment_1018_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 你这样的recentposts就是给自己看看的，要想给别人看还需签名发布出去。问题是人家为什么要接收你的recentposts?假设10万人每个人都签名发布自己的recentposts，需要多少空间你自己算吧想要有那个Everyone的效果，其实有很多方法，比如下载你朋友的朋友的data.json，相当于推荐机制，更加靠谱。或者向已连接节点请求最新被签名的data.json，也能得到陌生人的po。完整性虽有欠缺，但是推荐陌生人本来就不需要完整性。\n。。。你还是没有理解我的意思。现在每个人发送消息时都是签名自己的data.json，即所有消息，我说的是同时把单独消息本身来签名，发送给peers，你又说到空间，我之前也说过很多次了，每个节点收到这个请求后，会只存储最近的x条，以消息的时间为准删掉旧的消息条目，最后在本地组合为一个文件recentposts。",
				"added": 1481603230
			},
			{
				"comment_id": 25,
				"body": "> [lmath](#comment_320_1958F7oCppj78MP966AfojMQwHg2WUupzq): ZeroMe 的刷不出老评论那个 bug 到底该怎么修？单纯增加超时？那么以后评论数继续增加怎么办？或者给个提示“如果你想显示更多评论，可能会导致 ZeroNet 失去响应”？\n点进去PO主的profile page，找到这个消息，可以看到完整的评论，说明只是zerome中ui相关代码的问题。",
				"added": 1481604355
			},
			{
				"comment_id": 26,
				"body": "> [p2p](#comment_1021_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 你这样跟一条消息一个json有什么本质区别？全部是optional files？你的想法里到底哪些是必须下载的哪些是optional files？麻烦理清楚，完整地表述一次\n\n本质是一条消息一个json，但不必需分开存储（分开其实也可以），只要保证签名与验证的数据是单个消息就可以。recent posts全部都不是optional files，不过可以给用户选择希望接收的最多recentposts个数，就类似现在右侧拉开可以设定的size limit。\n现在zerome只有图片这种比较大的资源是optional files，需要主动请求下载，继续这样就可以。",
				"added": 1481651128
			},
			{
				"comment_id": 27,
				"body": "> [p2p](#comment_1022_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 既然你已经认定recent posts全部都不是optional files，不管是单个消息还是怎样，我可以明确地告诉你：如果hub人数达到10万数量级，想要保证首次加载下载数据量不超过10MB，非optional files不可能存放任何帖子（连一个帖子都放不下！），只能勉强存ping.json之类的东西。（10MB/100000=10000KB/100000=0.1KB=100Byte）\n建议你回去再看一遍我的回复，不需要首次加载，只需要在任何人post的时候，签名并推送，任何人收到后拿发送者的公钥验证这个信息，无误后更新本地的recentposts，10M(条)是在本地的一个预设值，如果超出限制，则删掉旧的消息，根本不存在分摊。\n外加刚上线时主动向peers请求新recentposts的，来更新自己的。\noptional files只是一种让用户对大文件选择性下载的机制，其实跟这个无关。\n你这样想，我这个recentposts的方案与现在最大不同的地方就是，粒度更细，用户发送消息时在签名自己的data.json的同时，把消息本身也签名，data.json照旧推送，只不过是改成只被订阅的用户接收，而单个消息的json是所有人都接收，现在其实本质就是这样的。\n你看我对比一下，\n现在是任何人发消息时签名data.json并推送给所有人，所有人都存储其他所有人的data.json。\n改成\n任何人发消息时签名消息本身推送给所有人，所有人存储**这条消息本身**到recentposts下，同时如果本地recenposts的条数多于x,则删掉最旧的。（这里细节上要考虑消息的回复或删改，也都是很简单的逻辑）\n同时照旧推送data.json给所有人，但是只有订阅了这个poster的用户才会**完整存储**他的data.json。\n会发现，本质与现在没有区别，不过存储的数据大小是可控的（不依赖于所有用户）。\n>请仔细思考，避免无意义地重复纠缠在这种显然的问题上。\n不是我纠缠，我只是在重复地解释我所表达的意思，因为你貌似总是曲解它，另外不清楚我们在这个post的讨论是否真的会有意义。",
				"added": 1481717200
			},
			{
				"comment_id": 28,
				"body": "另一个更简单的方案是：依旧以人为单位存储，命名为recentpeers按消息的时间控制存储的个数。\n即任何人发消息时照旧push data.json，所有人接收到后：\n\n如果订阅了该用户，则更新他的data.json\n否则如果\na) 此人在回复某最新的post\nb) 此人在发新的post，且本地recentpeers个数不足x\n则直接存储此人的data.json\n如果此人在发新的posts，且本地recentpeers超过x，则先删掉最旧发言且未参与之前posts的peer的data.json，再存储此人的data.json。\n其他情况则忽略请求。\n\n这个方案不需要做出太多改变，但是每个人存储的数据量还是依赖于最近发post的人与回复他的人本身，粒度不如仅依赖于消息细。",
				"added": 1481720877
			},
			{
				"comment_id": 29,
				"body": "> [p2p](#comment_1023_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 请在脑中完整地过一遍首次加载hub流程。首次加载是不是要请求一次所有人的recentposts?哪怕recentposts每人只有1条，10万人，传输数据量爆出10MB。（请注意，非optional files都会在首次加载时下载）\n\n~~要解决这个问题肯定要改zeronet的，如针对类似网站增加另一种模式，所以在本地可以做出消息条数限制。~~\n现在也不是请求所有数据，现在站点对于非可选文件也是可以设置size limit的。\n其实不改都可以，在每次请求消息后看大小是否超出，是的话就开始删旧的。",
				"added": 1481804911
			},
			{
				"comment_id": 30,
				"body": "> [p2p](#comment_1024_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 现在设置size limit不是你想的那种删旧的用途，而是防止站长把站搞得过大而伤害不知情的用户。非optional files有如下基本特征：\n> \n> 技术上或内容上必须要保持完整性，网站才正常\n> 要保持完整性，必须在首次加载时全部下载\n> 为了加快首次加载，必须每个节点都有一份完整的相同的拷贝\n> \n> 请问你的recentposts符合吗？为了解决陌生人推荐这种比较边缘的、不需要完整性的问题，把recentposts硬生生地塞进宝贵的非optional files里，引出更多的问题，实在得不偿失。\n看来有必要修改zeronet，因为直接下载所有数据这种运作方式是问题根本。像zerome这种网站，不需要保证完整性，可以把数据看成一个流，随时间流动，其余只有用户关心的数据才会被用户存储，所有用户都不关心的数据就会自动消亡。所以recentposts机制并没有对zerome引来更多问题。\n其实类似地还可以实现更有价值的topposts，只需综合考虑时间与like的个数以某种算法排序即可。\n>我之前说过，解决陌生人推荐有其他多种方法。\n此机制并不在于解决陌生人推荐的问题，而是给用户提供一个宏观的zerome网络的概念，即把整个网络即时的消息展现出来，陌生人推荐是另一个问题，可以通过同步用户follow的用户所follow的用户来实现。",
				"added": 1481862310
			},
			{
				"comment_id": 31,
				"body": "> [linkerlin](#comment_54_13Nzuvnhyz7nBJvPoUrdTEFuQTWUdqXAJk): 我觉得要换个思路，最好还是把每个Peer看成只持有一小块的图的节点，也就是持有一小块拼图，但是持有整个拼图的宏观描述，也就是持有一个Index来检索暂时没有持有的拼图。当网站在本机运行的时候，动态下载拼图。\n\nindex如何存储是个问题，如果每个index存储的是一个消息的hash，那么其实与没有Index是一样的，因为一个消息可以很短，比一个hash字符串要短，所以至少要多个消息hash一次或采用多级hash，问题是，如何保证hash的数据的真实性，如果是单独的一个消息，因为是发送者签名的可以验证，但多个消息或多个hash集合的数据本身是无法验证的。我认为旧的数据可以通过扩散地在zerome网络中同步实现，如A请求在时间X之前的数据，那他就向自己的peers发出这个请求，收到请求的查看自己存储的所有peers的data.json与recentposts，回复满足条件的消息，并继续扩散这个请求。旧数据的存储得用某种机制来保证在所有节点中均匀分摊，如此有助于搜索更旧消息。",
				"added": 1481866008
			},
			{
				"comment_id": 35,
				"body": "> [p2p](#comment_1026_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 不需要保证完整性？比如我一个月没上zeronet，别人当时@我，一个月之后上来看不到了？这是什么体验？\n断章取义，仔细看：\n可以把数据看成一个流，随时间流动，**其余**只有用户关心的数据才会被用户存储，**所有**用户**都不**关心的数据就会自动消亡\n你所举的例子，别人@我的情况，这个数据至少会存在于对方那里，并当这个事件发生时，你会得到他的数据拷贝（或仅这条消息）。\n> 给用户提供一个宏观的zerome网络的概念，那这问题就更边缘了。我之前说的向已连接节点请求最新被签名的data.json不就给用户提供一个宏观的zerome网络的概念了吗？反正不需要完整性。而且不会像你说的那种方案增加复杂度和性能负担。\n你所说的方案无法体现**宏观**，或全局。",
				"added": 1482234168
			},
			{
				"comment_id": 36,
				"body": "> [p2p](#comment_1028_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 我一个月没上zeronet，别人当时@我，我不在线；而等我上线，按照你的方案，我并不知道他有@过我，我自然不会去下载他的帖子，怎么办？（我之前跟此人没有任何交集）\n如此，发送者在发送后要请求回复，确认对方收到，若未得到确认，则定时推送，最终，所有的消息都会被确认收到时，就不需要定时推送了。\n> 我连接的节点是随机的，怎么不宏观？你一定要说全局，好像你的方案也无法保证完整性吧\n虽无法保证完整性，但可以展现所有参与zerome网络的用户的即时状态，我认为这是社交网络区别于blog用户互相往来的比较重要的一点。",
				"added": 1482241596
			},
			{
				"comment_id": 37,
				"body": "> [p2p](#comment_1029_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 这很像bitmessage的方式吧，很多人吐槽这种方式。我和他上线时间刚好错开怎么办？我白天他晚上\n这还真是一个问题，bitmessage是如何解决的？像被动关注这种情况不下载所有数据，同步起来的确有些麻烦。我在想，或许可以把每个人当作中转，帮忙转发一定量的消息，不过是一次性的，转发后在本地删除。其实现在的zeromail也存在类似问题（每个人都下载所有人的数据）。\n> 你会去看twitter所有人的最新信息流（假如有这个功能的话）吗？自己关注的人都看不过来（我现在在zerome就有点看不过来了）。你说一定要看所有人，好，我也给出了方便实现、消耗资源小的方案。但是像你的那种方案，为了提升一点这个边缘功能，对原架构大改，要费九牛二虎之力，得不偿失。\n打开twitter.com，未登录状态，会有\"what's happening\"，这就很类似，只不过twitter实现的策略更复杂。",
				"added": 1482244353
			}
		],
		"1_1DF7Sa6AkoLxZNPq5FSn3kAadKHfye9K6P": [
			{
				"comment_id": 19,
				"body": "用dtach或者screen/tmux",
				"added": 1481551489
			},
			{
				"comment_id": 33,
				"body": "> [zerocatty](#comment_8_1DcMmKhgdfkR1GDA6zhWhoFRreaej4wLmW): 还有第三个选择么?\n\n如果不关心输出, 试试python zeronet.py >/dev/null 2>&1 </dev/null &",
				"added": 1482000598
			}
		],
		"5_1DcMmKhgdfkR1GDA6zhWhoFRreaej4wLmW": [
			{
				"comment_id": 32,
				"body": "理想的生活是没有理想",
				"added": 1482000065
			}
		],
		"5_1NL5YZdozXc6ZNzKm78H4SjSMabyzV16YX": [
			{
				"comment_id": 34,
				"body": "改成tag的形式，可以选择只显示某些tag的内容，默认是混杂的，这样或许会好些。",
				"added": 1482049269
			}
		]
	},
	"comment_vote": {}
}