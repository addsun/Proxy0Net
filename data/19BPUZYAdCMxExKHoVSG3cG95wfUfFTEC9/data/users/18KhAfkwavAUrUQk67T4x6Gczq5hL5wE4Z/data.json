{
	"next_topic_id": 2,
	"topic": [
		{
			"topic_id": 1,
			"title": "当网站在不断变大时，zeronet最终将会陷入尴尬境地",
			"body": "我想把中国知网给镜像到zeronet，但是就存在以下问题，而且这些问题随着站点的变大以后都会在zeronet中出现，因此，如何解决这些问题是zeronet能否发长下去的关键，否则只能成七秒鱼。\n\n1. 按照现有API，搜索文档时将会查询本地数据库，而我离线下来的医学文献光 感染性疾病及传染病 部分就有3.7GB，若是要搜索这方面的文献每个人都要下载这3.7个G的数据才能检索。\n\n2. 有人会提到option file的模式，但写过论文的人都知道很多时候标题和摘要中都没有写那一方面的东西，但却出现在正文中，因此若要搜索全部还是得把这3.7个G下完。\n\n3. 医学中很多东西都是互相关联，或者无明确边界的，就如腹疼可能是心脏病的征兆，若是到消化系统疾病中查根本无法治疗，还会耽误治疗时间，因此对于一个医生来说最保险的方法是把知网所有医学数据库下完。（目测至少数千G）\n\n4. 其他方面也会有这样的问题，如艺术和历史、文学基本分不开的，那几块目前我也动不了，没地方存。\n\n5. 若是提出option file 把过往数据抛弃，首先不得不肯定抛弃历史巨人的做法是脑残解决办法中的战斗机，但是，你知道一年知网数据有多少吗？光2016年的就有 3,917,248 篇，按照普通无图文献150kb每份的均值来算还要560GB，若包括多媒体文献更是要成倍增长，更不要说里面数以千万计的扫描本PDF，这样的数据怎么去中心化？\n\n我能想到的解决办法：组网搜索，分布式处理数据\n\n由5台以上的计算机做种大战，分布式持有这个站点的数据，同时数据分布之后至少有两套备份，搜索时通过一套完整数据持有者进行检索，之后将信息反馈给检索者。也就是p2p下的云计算。\n\n虽然这样去中心化少了，但却可能是唯一的解决方法，不然现有框架下每人先去下数千G的数据再来搜索，这不扯的吗？\n\n可能我后面的解释有人没太懂，这里引用@lmath 的\n> 把大量数据分隔成小块数据，然后存储在不同的节点，在请求查询时给存储各个数据各个部分的节点都发送一个请求，每个节点只负责请求属于自己的那一部分并返回，最终由本地客户端合并结果\n这样来实现数据搜索",
			"added": 1480820256,
			"parent_topic_uri": "8_1NV87VmSkezRyz8NbFpdkYJsJUSfXx7LJa"
		}
	],
	"topic_vote": {},
	"next_comment_id": 28,
	"comment": {
		"1_18KhAfkwavAUrUQk67T4x6Gczq5hL5wE4Z": [
			{
				"comment_id": 1,
				"body": "> [mian](#comment_4_1BxAuooVQiHjjLbkPF8CPVoisJR3g2dEoz): 这个问题不是可以通过把一个站点分成多个站点来解决么？\n\n一个站点无论分成多少个站点，在现有API下使用搜索功能时都必须得全部下载下来，不然无法搜索",
				"added": 1480827965
			},
			{
				"comment_id": 2,
				"body": "> [p2p](#comment_980_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 大量数据的搜索很难去中心化的。你可以尝试提交给Sci-Hub：Sci-Hub是目前已知第一個提供大量自動且免費的付費學術論文的網站，使用者不需要事前訂閱或付款[11]，就能夠使用原本存放在付費資料庫的論文文章，並提供搜尋原先出版社網站內的文件檔案服務。其中這些檔案都是透過世界各地匿名學者提供的使用帳號，透過這些帳號使得網站能夠取得JSTOR、施普林格科學+商業媒體、賽吉出版公司、愛思唯爾等出版社所擁有的論文，並將之加以集結。當用戶向Sci-Hub提交論文請求時，網站會先搜尋創世紀圖書館是否有可提供的檔案；如果創世紀圖書館並沒有事先儲存，Sci-Hub會以特定資料庫的帳號下載文章，並送至創世紀圖書館備份以供將來使用。https://en.wikipedia.org/wiki/Sci-Hubhttps://zh.wikipedia.org/wiki/Sci-Hub\n\n但那是中心化服务，sci-hub一倒/被起诉强制关闭 一样没了。在zeronet就是要去中心化。",
				"added": 1480835557
			},
			{
				"comment_id": 3,
				"body": "> [swulling](#comment_9_1DzqZK5FdLSmsApSwT8Z56SmZDoeD34FJQ): 解决办法可能是吧Index和Src分开。Index就是倒排索引\n\n能否给出更多信息或参考链接？",
				"added": 1480835584
			},
			{
				"comment_id": 4,
				"body": "> [lmath](#comment_247_1958F7oCppj78MP966AfojMQwHg2WUupzq): 能否把大量数据分隔成小块数据，然后存储在不同的节点，在请求查询时给存储各个数据各个部分的节点都发送一个请求，每个节点只负责请求属于自己的那一部分并返回，最终由本地客户端合并结果呢？Sci-Hub 的问题 @generate 已经说了，主要是它是中心化存储的，容易由于某种原因而关闭。\n\n我的意思就是如此",
				"added": 1480838249
			},
			{
				"comment_id": 5,
				"body": "> [swulling](#comment_10_1DzqZK5FdLSmsApSwT8Z56SmZDoeD34FJQ): 一个简单的说明：http://blog.csdn.net/chunlei_zhang/article/details/38520315\n> Google/Baidu后面的技术也是类似的，Baidu 2012年的技术是每周创建一个分层倒排索引库，量小到可以加载到上千台机器的内存中（如果是原始网页，需要上千台机器的硬盘进行存储）。\n> 这个机制可能需要ZeroNet支持，开源的有Solr等\n\n我觉得这个可以给zeronet提个issue",
				"added": 1480838305
			},
			{
				"comment_id": 6,
				"body": "> [p2p](#comment_983_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): 我刚才改了一下，应该是PB级的数据。二级索引放到P2P网络上，不可靠。某个节点出问题了，没有完整返回信息，会导致搜索结果不可靠。不如换个思路，让知网的中心化搜索作嫁衣，只要搜索出文献编号，就用文献编号去P2P网络中搜寻，这样就把问题变成分布式存储（而不是之前的去中心化的搜索引擎）\n\n分布式存储又该怎么设计呢？",
				"added": 1480842783
			},
			{
				"comment_id": 7,
				"body": "> [p2p](#comment_984_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): optional files可以做很大的站\n> \n> optional files啊，zeronet现在已经支持分目录做种optional files（zerohello首页左侧文件标签），比如医学下各个分类分别由不同节点做种，单节点所需空间控制在100GB以内，我想还是有人愿意做种的。\n\n思考了一下，觉得不行。毕竟中国知网的数据库以及文献编号都不是统一固定的，若是中国知网的数据库变动那么一样无法检索到资源。另外，这里只能做p2p分布式存储的话便失去了原来的意义，不如ipfs。",
				"added": 1480859559
			},
			{
				"comment_id": 8,
				"body": "> [p2p](#comment_984_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ): optional files可以做很大的站\n> \n> optional files啊，zeronet现在已经支持分目录做种optional files（zerohello首页左侧文件标签），比如医学下各个分类分别由不同节点做种，单节点所需空间控制在100GB以内，我想还是有人愿意做种的。\n\n现在的option file模式以及merge site模式我认为都不够好，势必会导致热门文献节点无数，冷门的节点寥寥无几，如何均衡的分配节点做种是个问题，应该想出另外的办法来",
				"added": 1480859699
			}
		],
		"51_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": [
			{
				"comment_id": 9,
				"body": "> [linkerlin](#comment_39_13Nzuvnhyz7nBJvPoUrdTEFuQTWUdqXAJk): 发明一个类似Neo4J的图数据库，而且是分布式的。展开来讲：就是每个Peer可以存储一张巨大的Graph的一部分，包含 node、edge、property，其中 property就是一个dict。edge必须从一个node指向一个node.每个Peer等于是一个Cache，存储啦自己需要的图的一个碎片。\n\n这个是个好主意，也就是我之前所说的IPDB",
				"added": 1481379423
			},
			{
				"comment_id": 10,
				"body": "我觉得。。现在靠压缩还可以顶一段时间，刚刚把1Blue给zip压缩了一下，数据大小变为原来的50.6%，但是当数据越来越多这个方法也是不行的",
				"added": 1481379594
			},
			{
				"comment_id": 11,
				"body": "另外，zerome刷不出老评论不是bug，是设计时故意的，为的是提高加载速度",
				"added": 1481379646
			},
			{
				"comment_id": 12,
				"body": "> [cnwfjswr01](#comment_5_1Q5jU3rMt6F8ucYLpDKPzs13rzr49iqpGS): 可能是重复数据太多（\n\n毕竟格式化json空格都不知道有多少",
				"added": 1481379693
			},
			{
				"comment_id": 13,
				"body": "optionfile更多的是要浏览才去下载，而那些非option的大了。。丢option？还是其他解决办法？",
				"added": 1481379973
			},
			{
				"comment_id": 14,
				"body": "> [winarm](#comment_3_1GKFsYgkrTp4xYAxMP5x7artq7yoTKKCbS): 我就是这个意思，但是现在的问题好像是用户没有【拒绝做种】的能力\n\n【拒绝做种】的能力 的确需要",
				"added": 1481380021
			},
			{
				"comment_id": 15,
				"body": "> [rainlime](#comment_57_1EFRUQjydLVFKNFBiF2LW5rpQeqEr8EA6M): 定期让旧内容变为 Optional 啊\n\n还是那句老话，图书馆怎么办？",
				"added": 1481380041
			},
			{
				"comment_id": 16,
				"body": "> [rainlime](#comment_59_1EFRUQjydLVFKNFBiF2LW5rpQeqEr8EA6M): 诶 这和图书馆有关系么？\n> 要全部数据可以自行下载全部 Optional Files 啊\n\n详见这贴：http://127.0.0.1:43110/gfwtalk.bit/?Topic:1_18KhAfkwavAUrUQk67T4x6Gczq5hL5wE4Z/+zeronet",
				"added": 1481380201
			},
			{
				"comment_id": 17,
				"body": "zn需要更高效的数据存取以及搜索方式，原先我跑过脚本把所有站点所有文件都离线到本地，最后电脑卡死了，搜索一次没有一分半出不来",
				"added": 1481380313
			},
			{
				"comment_id": 18,
				"body": "当数据变得更多时该怎么办？",
				"added": 1481380335
			},
			{
				"comment_id": 19,
				"body": "hub大了可以把历史数据丢到option files，但其他的要被索引搜索的数据呢？",
				"added": 1481380384
			},
			{
				"comment_id": 20,
				"body": "> [lmath](#comment_280_1958F7oCppj78MP966AfojMQwHg2WUupzq): 话说你们在本论坛回复的时候有没有看到随机出现的分隔黑线？\n\nwin7x64+chrome 56.0.2924.18 dev (64-bit)最新版没有发现随机出现分割黑线，不过每刷新出一个新帖整个页面都会跳动一下",
				"added": 1481380586
			},
			{
				"comment_id": 21,
				"body": "> [winarm](#comment_6_1GKFsYgkrTp4xYAxMP5x7artq7yoTKKCbS): 不太懂……我的意思是同一个 hub 里的所有内容共享一个 “archive 时间”，执行 archive 的是客户端，但不确定当前是否能做到。\n> 或者换一个做法，把所有的 post 全部变成 optional，只自动将 “alive period” 内的 post 存到本地。超过  “alive period”  且未被 “fav” 的信息自动从本地删除。\n> \n> 中国知网什么的可能不适合拖到 0net 里……\n\n分布式存储如ipfs，storj，都已经出来了，分布式搜索引擎也有了yacy，为什么zeronet不可以？",
				"added": 1481380788
			},
			{
				"comment_id": 22,
				"body": "> [lmath](#comment_282_1958F7oCppj78MP966AfojMQwHg2WUupzq): 因为目前这个问题还不是很严峻，而且只有一个人在主要开发，只能走一步算一步了。\n\n唉。。。hub的老数据丢option，社交网络过时言论丢失了一两条还没啥关系。。但是那些真正有价值的数据丢了。。唉。。走一步算一步走一步算一步",
				"added": 1481381238
			},
			{
				"comment_id": 23,
				"body": "> [linkerlin](#comment_40_13Nzuvnhyz7nBJvPoUrdTEFuQTWUdqXAJk): 目前的optional机制不够自动化。\n\n嗯，我觉得应该设置一套自律规则让zn自动把标识为社交站的过时数据变成optionfile，但是。。还是先解决如何防御spam的问题吧",
				"added": 1481381379
			},
			{
				"comment_id": 24,
				"body": "> [lmath](#comment_284_1958F7oCppj78MP966AfojMQwHg2WUupzq): Nofish 的方案是：封号。由于 ZeroID 还是相对中心化的（可以注册时加验证码什么的措施），所以不会有太大问题。\n\n封号？我先慢慢偷偷摸摸的注册999999个号，之后在zn上到处spam，然后。。一个个人工审核封号？",
				"added": 1481381689
			},
			{
				"comment_id": 25,
				"body": "> [linkerlin](#comment_44_13Nzuvnhyz7nBJvPoUrdTEFuQTWUdqXAJk): 可以搞个协作式的黑名单。大家共享统一份黑名单。只要有N个人把同一个人加入黑名单，大家都信任这个结果。也就是说，有个负面积分系统。\n\n# 一起去闹腾zerome的block功能吧\n这里已经有提议了，[传送门](https://github.com/HelloZeroNet/ZeroMe/issues/9)\n### 一起留言去吧~",
				"added": 1481382274
			},
			{
				"comment_id": 26,
				"body": "# 一起去闹腾zerome的block功能吧\n这里已经有提议了，[传送门](https://github.com/HelloZeroNet/ZeroMe/issues/9)\n### 一起留言去吧~",
				"added": 1481382280
			},
			{
				"comment_id": 27,
				"body": "> [linkerlin](#comment_44_13Nzuvnhyz7nBJvPoUrdTEFuQTWUdqXAJk): 可以搞个协作式的黑名单。大家共享统一份黑名单。只要有N个人把同一个人加入黑名单，大家都信任这个结果。也就是说，有个负面积分系统。\n\n这个提议就是这个想法，一起去提意见吧 [传送门](https://github.com/HelloZeroNet/ZeroMe/issues/9)",
				"added": 1481382314
			}
		]
	},
	"comment_vote": {}
}